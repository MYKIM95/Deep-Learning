{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brief-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stainless-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "theoretical-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('data/wine.csv')\n",
    "\n",
    "y = wine.iloc[:, -2]   # y = wine.iloc[:, 11]\n",
    "del wine['quality'] # 정답컬럼 삭제\n",
    "X = wine\n",
    "\n",
    "y_en = pd.get_dummies(y) # 정답컬럼을 7개로 원핫인코딩\n",
    "\n",
    "\n",
    "# 출력층의 활성화함수는 softmax로 설정해주자.\n",
    "# compile loss = 'categorical_crossentropy'\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim = 12, activation = 'relu')) # 입력층\n",
    "model.add(Dense(48, activation = 'relu'))  # 은닉층1\n",
    "model.add(Dense(24, activation = 'relu'))  # 은닉층2\n",
    "model.add(Dense(7, activation = 'softmax')) # 출력층\n",
    "\n",
    "# softmax 다중분류시 쓰는 활성화함수\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-washer",
   "metadata": {},
   "source": [
    "### 1. 모델 실행 및 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "impossible-watershed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "south-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "leading-transparency",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.32259, saving model to ./model2/01 - 1.3226.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.32259 to 1.30002, saving model to ./model2/02 - 1.3000.hdf5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.30002\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.30002\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.30002\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.30002\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.30002\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.30002 to 1.29319, saving model to ./model2/08 - 1.2932.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.29319 to 1.27718, saving model to ./model2/09 - 1.2772.hdf5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.27718\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.27718\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.27718\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.27718 to 1.26709, saving model to ./model2/13 - 1.2671.hdf5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.26709\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.26709\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.26709\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.26709\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.26709\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.26709 to 1.23772, saving model to ./model2/19 - 1.2377.hdf5\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.23772 to 1.23349, saving model to ./model2/20 - 1.2335.hdf5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.23349\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.23349\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.23349 to 1.23087, saving model to ./model2/23 - 1.2309.hdf5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.23087\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.23087\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.23087 to 1.21585, saving model to ./model2/26 - 1.2158.hdf5\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.21585 to 1.19197, saving model to ./model2/27 - 1.1920.hdf5\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.19197 to 1.17191, saving model to ./model2/28 - 1.1719.hdf5\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.17191 to 1.15233, saving model to ./model2/29 - 1.1523.hdf5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.15233\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.15233\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.15233 to 1.14045, saving model to ./model2/32 - 1.1404.hdf5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.14045\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.14045 to 1.13479, saving model to ./model2/34 - 1.1348.hdf5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.13479\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.13479\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.13479\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.13479 to 1.13100, saving model to ./model2/38 - 1.1310.hdf5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.13100\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.13100 to 1.12324, saving model to ./model2/40 - 1.1232.hdf5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.12324\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.12324\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.12324 to 1.12248, saving model to ./model2/43 - 1.1225.hdf5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.12248\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.12248\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.12248\n",
      "\n",
      "Epoch 00047: val_loss improved from 1.12248 to 1.10955, saving model to ./model2/47 - 1.1096.hdf5\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.10955\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.10955\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.10955\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.10955\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.10955\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.10955\n",
      "\n",
      "Epoch 00054: val_loss improved from 1.10955 to 1.10210, saving model to ./model2/54 - 1.1021.hdf5\n",
      "\n",
      "Epoch 00055: val_loss improved from 1.10210 to 1.09511, saving model to ./model2/55 - 1.0951.hdf5\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.09511\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.09511\n",
      "\n",
      "Epoch 00058: val_loss improved from 1.09511 to 1.08533, saving model to ./model2/58 - 1.0853.hdf5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.08533\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.08533\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.08533\n",
      "\n",
      "Epoch 00062: val_loss improved from 1.08533 to 1.08515, saving model to ./model2/62 - 1.0851.hdf5\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.08515\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.08515\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.08515\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.08515\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.08515\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.08515\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.08515\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.08515\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.08515\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.08515\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.08515\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.08515\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.08515\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.08515\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.08515\n",
      "\n",
      "Epoch 00078: val_loss improved from 1.08515 to 1.06708, saving model to ./model2/78 - 1.0671.hdf5\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.06708\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.06708\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.06708\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.06708\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.06708\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.06708\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.06708\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.06708\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.06708\n",
      "\n",
      "Epoch 00088: val_loss improved from 1.06708 to 1.06700, saving model to ./model2/88 - 1.0670.hdf5\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.06700\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.06700\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.06700\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.06700\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.06700\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.06700\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.06700\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.06700\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.06700\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.06700\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.06700\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.06700\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.06700\n",
      "\n",
      "Epoch 00102: val_loss improved from 1.06700 to 1.06502, saving model to ./model2/102 - 1.0650.hdf5\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1.06502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00139: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 1.06502\n",
      "\n",
      "Epoch 00213: val_loss improved from 1.06502 to 1.06395, saving model to ./model2/213 - 1.0640.hdf5\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 1.06395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00296: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 1.06395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00454: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 1.06395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00612: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 1.06395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00771: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 1.06395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00930: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 1.06395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01090: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01219: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 1.06395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01250: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01270: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01282: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01315: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01325: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01352: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01353: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01355: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01381: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01393: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01395: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01407: val_loss did not improve from 1.06395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01408: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01422: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01428: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01429: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01431: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01475: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01484: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01496: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01497: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01498: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01499: val_loss did not improve from 1.06395\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 1.06395\n"
     ]
    }
   ],
   "source": [
    "# 현재 폴더 탐색\n",
    "MODEL_DIR = './model2'\n",
    "\n",
    "# 폴더가 없다면 폴더 생성\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "# 저장할 파일명의 정규식 설정 {epoch의 수 - 손실값을 파일명으로 설정}    \n",
    "modelpath = MODEL_DIR + \"/{epoch:02d} - {val_loss:.4f}.hdf5\"\n",
    "\n",
    "# filepath : 저장한 파일경로\n",
    "# monitor : 체크할 값(모니터할 값을 설정 (val_loss : 오차, val_Acc : 정확도, loss : 훈련오차))\n",
    "# save_best_only(True) : 이전에 저장한 모델보다 나은 경우에만 저장\n",
    "checkpointer = ModelCheckpoint(filepath = modelpath, monitor = 'val_loss',\n",
    "                              save_best_only = True, verbose = 1)\n",
    "\n",
    "# validation_split = 0.33 : 33%만 테스트셋으로 사용\n",
    "# callbacks 모델 시행중에 콜백함수 호출\n",
    "history = model.fit(X, y_en, validation_split = 0.33, epochs = 1500, batch_size = 100, verbose = 0, callbacks = [checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "overall-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_model = tf.keras.models.load_model('model2/213 - 1.0640.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "numerical-complaint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6497/6497 [==============================] - 0s 48us/sample - loss: 1.0470 - accuracy: 0.5492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0469869315046263, 0.5491765]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_model.evaluate(X, y_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "qualified-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "opposed-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_acc = history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wired-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = np.arange(len(y_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-sucking",
   "metadata": {},
   "source": [
    "### 2. 그래프로 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "separate-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "indoor-sentence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_len, y_loss, c = \"red\", markersize = 3)\n",
    "plt.plot(x_len, y_acc, c = \"blue\", markersize = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-makeup",
   "metadata": {},
   "source": [
    "### 3. 자동 학습 중단"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-chemical",
   "metadata": {},
   "source": [
    "#### 3.1  EarlyStopping - 테스트의 오차가 줄지 않으면 테스트를 멈추는 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "proprietary-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "weird-designer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5197 samples, validate on 1300 samples\n",
      "Epoch 1/2000\n",
      "5197/5197 [==============================] - 0s 12us/sample - loss: 0.9433 - accuracy: 0.6065 - val_loss: 1.4277 - val_accuracy: 0.4877\n",
      "Epoch 2/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9265 - accuracy: 0.6094 - val_loss: 1.4504 - val_accuracy: 0.4677\n",
      "Epoch 3/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9165 - accuracy: 0.6111 - val_loss: 1.4036 - val_accuracy: 0.4669\n",
      "Epoch 4/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.9076 - accuracy: 0.6100 - val_loss: 1.3532 - val_accuracy: 0.4862\n",
      "Epoch 5/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9053 - accuracy: 0.6171 - val_loss: 1.3944 - val_accuracy: 0.4654\n",
      "Epoch 6/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.9017 - accuracy: 0.6177 - val_loss: 1.3489 - val_accuracy: 0.4754\n",
      "Epoch 7/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8992 - accuracy: 0.6196 - val_loss: 1.3441 - val_accuracy: 0.4715\n",
      "Epoch 8/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8970 - accuracy: 0.6169 - val_loss: 1.3413 - val_accuracy: 0.4900\n",
      "Epoch 9/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8976 - accuracy: 0.6198 - val_loss: 1.3282 - val_accuracy: 0.4923\n",
      "Epoch 10/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8953 - accuracy: 0.6194 - val_loss: 1.3354 - val_accuracy: 0.4877\n",
      "Epoch 11/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8931 - accuracy: 0.6171 - val_loss: 1.3242 - val_accuracy: 0.4892\n",
      "Epoch 12/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8926 - accuracy: 0.6152 - val_loss: 1.3159 - val_accuracy: 0.5000\n",
      "Epoch 13/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8935 - accuracy: 0.6169 - val_loss: 1.3038 - val_accuracy: 0.4977\n",
      "Epoch 14/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8919 - accuracy: 0.6200 - val_loss: 1.3161 - val_accuracy: 0.4892\n",
      "Epoch 15/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8927 - accuracy: 0.6184 - val_loss: 1.2985 - val_accuracy: 0.4977\n",
      "Epoch 16/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8888 - accuracy: 0.6204 - val_loss: 1.3304 - val_accuracy: 0.4885\n",
      "Epoch 17/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8915 - accuracy: 0.6142 - val_loss: 1.2797 - val_accuracy: 0.5077\n",
      "Epoch 18/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8929 - accuracy: 0.6173 - val_loss: 1.3063 - val_accuracy: 0.4969\n",
      "Epoch 19/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8895 - accuracy: 0.6188 - val_loss: 1.3108 - val_accuracy: 0.4985\n",
      "Epoch 20/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8892 - accuracy: 0.6177 - val_loss: 1.3171 - val_accuracy: 0.4931\n",
      "Epoch 21/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8882 - accuracy: 0.6223 - val_loss: 1.3261 - val_accuracy: 0.4731\n",
      "Epoch 22/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8904 - accuracy: 0.6159 - val_loss: 1.3545 - val_accuracy: 0.4623\n",
      "Epoch 23/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8960 - accuracy: 0.6105 - val_loss: 1.2934 - val_accuracy: 0.4892\n",
      "Epoch 24/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8877 - accuracy: 0.6173 - val_loss: 1.2698 - val_accuracy: 0.5085\n",
      "Epoch 25/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8873 - accuracy: 0.6213 - val_loss: 1.3223 - val_accuracy: 0.4877\n",
      "Epoch 26/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8867 - accuracy: 0.6154 - val_loss: 1.2752 - val_accuracy: 0.5038\n",
      "Epoch 27/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8864 - accuracy: 0.6171 - val_loss: 1.2953 - val_accuracy: 0.4915\n",
      "Epoch 28/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8866 - accuracy: 0.6144 - val_loss: 1.2951 - val_accuracy: 0.4985\n",
      "Epoch 29/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8864 - accuracy: 0.6213 - val_loss: 1.2819 - val_accuracy: 0.5077\n",
      "Epoch 30/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8856 - accuracy: 0.6171 - val_loss: 1.2891 - val_accuracy: 0.5008\n",
      "Epoch 31/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8855 - accuracy: 0.6182 - val_loss: 1.3220 - val_accuracy: 0.4869\n",
      "Epoch 32/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8853 - accuracy: 0.6150 - val_loss: 1.3293 - val_accuracy: 0.4746\n",
      "Epoch 33/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8863 - accuracy: 0.6190 - val_loss: 1.3328 - val_accuracy: 0.4808\n",
      "Epoch 34/2000\n",
      "5197/5197 [==============================] - 0s 8us/sample - loss: 0.8846 - accuracy: 0.6252 - val_loss: 1.3272 - val_accuracy: 0.4862\n",
      "Epoch 35/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8832 - accuracy: 0.6155 - val_loss: 1.3147 - val_accuracy: 0.4877\n",
      "Epoch 36/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8840 - accuracy: 0.6219 - val_loss: 1.3004 - val_accuracy: 0.4969\n",
      "Epoch 37/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8825 - accuracy: 0.6175 - val_loss: 1.3199 - val_accuracy: 0.4769\n",
      "Epoch 38/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8854 - accuracy: 0.6169 - val_loss: 1.3160 - val_accuracy: 0.4808\n",
      "Epoch 39/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8844 - accuracy: 0.6204 - val_loss: 1.3259 - val_accuracy: 0.4777\n",
      "Epoch 40/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8830 - accuracy: 0.6186 - val_loss: 1.3180 - val_accuracy: 0.4877\n",
      "Epoch 41/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8819 - accuracy: 0.6211 - val_loss: 1.3333 - val_accuracy: 0.4831\n",
      "Epoch 42/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8834 - accuracy: 0.6167 - val_loss: 1.3158 - val_accuracy: 0.4785\n",
      "Epoch 43/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8870 - accuracy: 0.6157 - val_loss: 1.3053 - val_accuracy: 0.4900\n",
      "Epoch 44/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8843 - accuracy: 0.6223 - val_loss: 1.3052 - val_accuracy: 0.4954\n",
      "Epoch 45/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8842 - accuracy: 0.6177 - val_loss: 1.3197 - val_accuracy: 0.4746\n",
      "Epoch 46/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8814 - accuracy: 0.6204 - val_loss: 1.3401 - val_accuracy: 0.4808\n",
      "Epoch 47/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8831 - accuracy: 0.6204 - val_loss: 1.3008 - val_accuracy: 0.4862\n",
      "Epoch 48/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8801 - accuracy: 0.6175 - val_loss: 1.3142 - val_accuracy: 0.4954\n",
      "Epoch 49/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8820 - accuracy: 0.6232 - val_loss: 1.3022 - val_accuracy: 0.4992\n",
      "Epoch 50/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8827 - accuracy: 0.6179 - val_loss: 1.3214 - val_accuracy: 0.4954\n",
      "Epoch 51/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8833 - accuracy: 0.6209 - val_loss: 1.2930 - val_accuracy: 0.4938\n",
      "Epoch 52/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8870 - accuracy: 0.6161 - val_loss: 1.3200 - val_accuracy: 0.4792\n",
      "Epoch 53/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8816 - accuracy: 0.6225 - val_loss: 1.3023 - val_accuracy: 0.5108\n",
      "Epoch 54/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8831 - accuracy: 0.6179 - val_loss: 1.2633 - val_accuracy: 0.5023\n",
      "Epoch 55/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8832 - accuracy: 0.6169 - val_loss: 1.3895 - val_accuracy: 0.4638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8845 - accuracy: 0.6177 - val_loss: 1.3153 - val_accuracy: 0.4908\n",
      "Epoch 57/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8839 - accuracy: 0.6140 - val_loss: 1.2849 - val_accuracy: 0.4992\n",
      "Epoch 58/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8789 - accuracy: 0.6177 - val_loss: 1.2860 - val_accuracy: 0.5062\n",
      "Epoch 59/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8822 - accuracy: 0.6217 - val_loss: 1.2927 - val_accuracy: 0.5108\n",
      "Epoch 60/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8804 - accuracy: 0.6198 - val_loss: 1.3227 - val_accuracy: 0.4869\n",
      "Epoch 61/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8814 - accuracy: 0.6184 - val_loss: 1.3068 - val_accuracy: 0.4931\n",
      "Epoch 62/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8838 - accuracy: 0.6202 - val_loss: 1.3145 - val_accuracy: 0.4808\n",
      "Epoch 63/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8801 - accuracy: 0.6192 - val_loss: 1.2878 - val_accuracy: 0.5108\n",
      "Epoch 64/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8782 - accuracy: 0.6173 - val_loss: 1.3018 - val_accuracy: 0.4908\n",
      "Epoch 65/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8774 - accuracy: 0.6231 - val_loss: 1.3088 - val_accuracy: 0.5038\n",
      "Epoch 66/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8788 - accuracy: 0.6211 - val_loss: 1.2759 - val_accuracy: 0.5169\n",
      "Epoch 67/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8804 - accuracy: 0.6171 - val_loss: 1.2866 - val_accuracy: 0.5085\n",
      "Epoch 68/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8838 - accuracy: 0.6163 - val_loss: 1.2760 - val_accuracy: 0.5169\n",
      "Epoch 69/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8824 - accuracy: 0.6175 - val_loss: 1.2931 - val_accuracy: 0.5054\n",
      "Epoch 70/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8824 - accuracy: 0.6188 - val_loss: 1.2975 - val_accuracy: 0.5062\n",
      "Epoch 71/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8789 - accuracy: 0.6186 - val_loss: 1.2731 - val_accuracy: 0.5092\n",
      "Epoch 72/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8799 - accuracy: 0.6244 - val_loss: 1.3079 - val_accuracy: 0.5062\n",
      "Epoch 73/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8783 - accuracy: 0.6200 - val_loss: 1.3114 - val_accuracy: 0.4800\n",
      "Epoch 74/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8774 - accuracy: 0.6207 - val_loss: 1.3013 - val_accuracy: 0.5023\n",
      "Epoch 75/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8771 - accuracy: 0.6202 - val_loss: 1.2959 - val_accuracy: 0.4931\n",
      "Epoch 76/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8774 - accuracy: 0.6200 - val_loss: 1.3312 - val_accuracy: 0.4762\n",
      "Epoch 77/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8792 - accuracy: 0.6194 - val_loss: 1.3131 - val_accuracy: 0.4854\n",
      "Epoch 78/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8786 - accuracy: 0.6232 - val_loss: 1.2856 - val_accuracy: 0.4931\n",
      "Epoch 79/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8810 - accuracy: 0.6159 - val_loss: 1.3249 - val_accuracy: 0.4723\n",
      "Epoch 80/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8796 - accuracy: 0.6179 - val_loss: 1.3115 - val_accuracy: 0.4923\n",
      "Epoch 81/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8762 - accuracy: 0.6242 - val_loss: 1.2962 - val_accuracy: 0.4962\n",
      "Epoch 82/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8772 - accuracy: 0.6207 - val_loss: 1.3003 - val_accuracy: 0.4946\n",
      "Epoch 83/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8769 - accuracy: 0.6225 - val_loss: 1.3045 - val_accuracy: 0.5092\n",
      "Epoch 84/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8757 - accuracy: 0.6196 - val_loss: 1.3080 - val_accuracy: 0.5000\n",
      "Epoch 85/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8777 - accuracy: 0.6242 - val_loss: 1.2819 - val_accuracy: 0.5008\n",
      "Epoch 86/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8764 - accuracy: 0.6250 - val_loss: 1.2774 - val_accuracy: 0.5062\n",
      "Epoch 87/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8817 - accuracy: 0.6196 - val_loss: 1.2872 - val_accuracy: 0.5162\n",
      "Epoch 88/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8829 - accuracy: 0.6190 - val_loss: 1.2803 - val_accuracy: 0.5262\n",
      "Epoch 89/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8857 - accuracy: 0.6115 - val_loss: 1.2728 - val_accuracy: 0.5238\n",
      "Epoch 90/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8810 - accuracy: 0.6171 - val_loss: 1.3180 - val_accuracy: 0.4938\n",
      "Epoch 91/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8774 - accuracy: 0.6202 - val_loss: 1.2831 - val_accuracy: 0.5077\n",
      "Epoch 92/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8772 - accuracy: 0.6159 - val_loss: 1.3386 - val_accuracy: 0.5000\n",
      "Epoch 93/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8750 - accuracy: 0.6206 - val_loss: 1.2971 - val_accuracy: 0.5023\n",
      "Epoch 94/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8759 - accuracy: 0.6244 - val_loss: 1.3270 - val_accuracy: 0.4923\n",
      "Epoch 95/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8742 - accuracy: 0.6221 - val_loss: 1.2902 - val_accuracy: 0.5015\n",
      "Epoch 96/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8763 - accuracy: 0.6200 - val_loss: 1.3176 - val_accuracy: 0.4915\n",
      "Epoch 97/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8786 - accuracy: 0.6217 - val_loss: 1.2841 - val_accuracy: 0.5031\n",
      "Epoch 98/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8767 - accuracy: 0.6252 - val_loss: 1.3009 - val_accuracy: 0.5123\n",
      "Epoch 99/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8770 - accuracy: 0.6202 - val_loss: 1.2857 - val_accuracy: 0.4900\n",
      "Epoch 100/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8760 - accuracy: 0.6206 - val_loss: 1.3085 - val_accuracy: 0.4892\n",
      "Epoch 101/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8739 - accuracy: 0.6213 - val_loss: 1.2877 - val_accuracy: 0.5054\n",
      "Epoch 102/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8738 - accuracy: 0.6232 - val_loss: 1.3200 - val_accuracy: 0.5046\n",
      "Epoch 103/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8792 - accuracy: 0.6244 - val_loss: 1.2721 - val_accuracy: 0.5100\n",
      "Epoch 104/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8773 - accuracy: 0.6180 - val_loss: 1.3254 - val_accuracy: 0.4992\n",
      "Epoch 105/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8740 - accuracy: 0.6236 - val_loss: 1.3233 - val_accuracy: 0.4823\n",
      "Epoch 106/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8784 - accuracy: 0.6179 - val_loss: 1.3218 - val_accuracy: 0.4915\n",
      "Epoch 107/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8744 - accuracy: 0.6252 - val_loss: 1.2827 - val_accuracy: 0.5138\n",
      "Epoch 108/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8750 - accuracy: 0.6217 - val_loss: 1.3092 - val_accuracy: 0.5177\n",
      "Epoch 109/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8756 - accuracy: 0.6213 - val_loss: 1.2875 - val_accuracy: 0.5062\n",
      "Epoch 110/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8785 - accuracy: 0.6171 - val_loss: 1.3249 - val_accuracy: 0.4746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8809 - accuracy: 0.6188 - val_loss: 1.3296 - val_accuracy: 0.4938\n",
      "Epoch 112/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8735 - accuracy: 0.6221 - val_loss: 1.3064 - val_accuracy: 0.5031\n",
      "Epoch 113/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8734 - accuracy: 0.6250 - val_loss: 1.3542 - val_accuracy: 0.4715\n",
      "Epoch 114/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8717 - accuracy: 0.6250 - val_loss: 1.3053 - val_accuracy: 0.5000\n",
      "Epoch 115/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8714 - accuracy: 0.6246 - val_loss: 1.2858 - val_accuracy: 0.5138\n",
      "Epoch 116/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8725 - accuracy: 0.6232 - val_loss: 1.3389 - val_accuracy: 0.4838\n",
      "Epoch 117/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8735 - accuracy: 0.6232 - val_loss: 1.3178 - val_accuracy: 0.4962\n",
      "Epoch 118/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8738 - accuracy: 0.6254 - val_loss: 1.2814 - val_accuracy: 0.5146\n",
      "Epoch 119/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8708 - accuracy: 0.6234 - val_loss: 1.3128 - val_accuracy: 0.4931\n",
      "Epoch 120/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8719 - accuracy: 0.6269 - val_loss: 1.3335 - val_accuracy: 0.4823\n",
      "Epoch 121/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8727 - accuracy: 0.6250 - val_loss: 1.3257 - val_accuracy: 0.4938\n",
      "Epoch 122/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8708 - accuracy: 0.6265 - val_loss: 1.3178 - val_accuracy: 0.4931\n",
      "Epoch 123/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8719 - accuracy: 0.6248 - val_loss: 1.2742 - val_accuracy: 0.5115\n",
      "Epoch 124/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8763 - accuracy: 0.6229 - val_loss: 1.3065 - val_accuracy: 0.4962\n",
      "Epoch 125/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8764 - accuracy: 0.6173 - val_loss: 1.3132 - val_accuracy: 0.4815\n",
      "Epoch 126/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8721 - accuracy: 0.6267 - val_loss: 1.3219 - val_accuracy: 0.4908\n",
      "Epoch 127/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8713 - accuracy: 0.6209 - val_loss: 1.3175 - val_accuracy: 0.4885\n",
      "Epoch 128/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8737 - accuracy: 0.6232 - val_loss: 1.2889 - val_accuracy: 0.5185\n",
      "Epoch 129/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8748 - accuracy: 0.6204 - val_loss: 1.2793 - val_accuracy: 0.5185\n",
      "Epoch 130/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8693 - accuracy: 0.6252 - val_loss: 1.3235 - val_accuracy: 0.4869\n",
      "Epoch 131/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8734 - accuracy: 0.6231 - val_loss: 1.2777 - val_accuracy: 0.5077\n",
      "Epoch 132/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8718 - accuracy: 0.6206 - val_loss: 1.3061 - val_accuracy: 0.5023\n",
      "Epoch 133/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8722 - accuracy: 0.6294 - val_loss: 1.3286 - val_accuracy: 0.4831\n",
      "Epoch 134/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8715 - accuracy: 0.6215 - val_loss: 1.2960 - val_accuracy: 0.5015\n",
      "Epoch 135/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8714 - accuracy: 0.6232 - val_loss: 1.2994 - val_accuracy: 0.5054\n",
      "Epoch 136/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8695 - accuracy: 0.6273 - val_loss: 1.3046 - val_accuracy: 0.5023\n",
      "Epoch 137/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8705 - accuracy: 0.6286 - val_loss: 1.3015 - val_accuracy: 0.4931\n",
      "Epoch 138/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8702 - accuracy: 0.6240 - val_loss: 1.3228 - val_accuracy: 0.4962\n",
      "Epoch 139/2000\n",
      "5197/5197 [==============================] - 0s 7us/sample - loss: 0.8728 - accuracy: 0.6213 - val_loss: 1.2824 - val_accuracy: 0.4962\n",
      "Epoch 140/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8706 - accuracy: 0.6250 - val_loss: 1.3318 - val_accuracy: 0.4838\n",
      "Epoch 141/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8722 - accuracy: 0.6231 - val_loss: 1.3160 - val_accuracy: 0.4900\n",
      "Epoch 142/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8820 - accuracy: 0.6217 - val_loss: 1.3274 - val_accuracy: 0.4662\n",
      "Epoch 143/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8745 - accuracy: 0.6194 - val_loss: 1.3486 - val_accuracy: 0.4677\n",
      "Epoch 144/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8720 - accuracy: 0.6273 - val_loss: 1.3382 - val_accuracy: 0.4746\n",
      "Epoch 145/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8752 - accuracy: 0.6198 - val_loss: 1.3292 - val_accuracy: 0.4708\n",
      "Epoch 146/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8779 - accuracy: 0.6238 - val_loss: 1.2936 - val_accuracy: 0.5023\n",
      "Epoch 147/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8737 - accuracy: 0.6207 - val_loss: 1.3352 - val_accuracy: 0.4846\n",
      "Epoch 148/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8686 - accuracy: 0.6232 - val_loss: 1.2815 - val_accuracy: 0.5177\n",
      "Epoch 149/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8739 - accuracy: 0.6261 - val_loss: 1.2814 - val_accuracy: 0.5277\n",
      "Epoch 150/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8822 - accuracy: 0.6163 - val_loss: 1.2980 - val_accuracy: 0.5223\n",
      "Epoch 151/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8754 - accuracy: 0.6213 - val_loss: 1.3072 - val_accuracy: 0.4877\n",
      "Epoch 152/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8751 - accuracy: 0.6219 - val_loss: 1.3152 - val_accuracy: 0.4854\n",
      "Epoch 153/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8727 - accuracy: 0.6211 - val_loss: 1.3070 - val_accuracy: 0.5023\n",
      "Epoch 154/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8710 - accuracy: 0.6273 - val_loss: 1.3312 - val_accuracy: 0.4838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2abf8f8b080>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epoch 마다 오차가 줄지 않아도 기다릴 횟수 -> patience\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 100)\n",
    "\n",
    "model.fit(X, y_en, validation_split = 0.2 , epochs = 2000, batch_size = 500, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-waters",
   "metadata": {},
   "source": [
    "### 4. 과적합방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "metropolitan-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "further-leisure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim = 12, activation = 'relu')) # 입력층\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(48, activation = 'relu'))  # 은닉층1\n",
    "model.add(Dropout(0.75))\n",
    "model.add(Dense(24, activation = 'relu'))  # 은닉층2\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(7, activation = 'softmax')) # 출력층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-rebate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
